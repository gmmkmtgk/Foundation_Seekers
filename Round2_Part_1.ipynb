{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                         # Has necessary modules for PoS Tagging\n",
    "from nltk.corpus import stopwords   # Contains stop words that need to be removed\n",
    "from nltk import pos_tag            # Used to perform PoS Tagging\n",
    "\n",
    "import regex as re                  # Used for matching words in the text\n",
    "import unidecode                    # To remove Greek accents\n",
    "\n",
    "from wordcloud import WordCloud     # To generate worcloud\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B1 is \"The Kingmakers, by Burton E. Stevenson.txt\"\n",
    "#B2 is \"The Return of Sherlock Holmes, by Sir Arthur Conan Doyle.txt\"\n",
    "#B3 is \"The Secret of Toni by Molly Elliot Seawell.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countess', 'remond', 'entering', 'diningroom', 'lounge', 'moment', 'halfhour', 'dinner', 'sporting', 'club', 'odours', 'coffee', 'tobacco', 'delicate', 'perfumes', 'clothes', 'hair', 'shoulders', 'women', 'chairs', 'women', 'scene', 'men', 'eye', 'donor', 'corner', 'picture', 'madonna', 'men', 'donors', 'things', 'resemblance', 'nothing', 'madonnalike', 'women', 'blonde', 'brune', 'contours', 'ages', 'quality', 'spirit', 'sisterhood', 'gowns', 'gamut', 'rainbow', 'material', 'degree', 'eccentricity', 'purpose', 'neck']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "#open text file in read mode\n",
    "text_file_B1 = open(\"B1.txt\", \"r\")\n",
    "data = text_file_B1.read()\n",
    "text_file_B1.close()\n",
    "# blob = TextBlob(data)\n",
    "# print(blob.noun_phrases[:100])\n",
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B1 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_list_B1_NN = nouns_B1[:] #make a copy of the word_list\n",
    "for word in nouns_B1: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B1_NN.remove(word) # remove word from filtered_word_list if it is a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countess', 'remond', 'entering', 'diningroom', 'lounge', 'moment', 'halfhour', 'dinner', 'sporting', 'club', 'odours', 'coffee', 'tobacco', 'delicate', 'perfumes', 'clothes', 'hair', 'shoulders', 'women', 'chairs', 'women', 'scene', 'men', 'eye', 'donor', 'corner', 'picture', 'madonna', 'men', 'donors', 'things', 'resemblance', 'nothing', 'madonnalike', 'women', 'blonde', 'brune', 'contours', 'ages', 'quality', 'spirit', 'sisterhood', 'gowns', 'gamut', 'rainbow', 'material', 'degree', 'eccentricity', 'purpose', 'neck']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_word_list_B1_NN[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spring', 'year', 'london', 'world', 'murder', 'ronald', 'adair', 'circumstances', 'public', 'particulars', 'crime', 'police', 'investigation', 'deal', 'occasion', 'case', 'prosecution', 'facts', 'end', 'years', 'links', 'whole', 'chain', 'crime', 'interest', 'interest', 'nothing', 'sequel', 'shock', 'surprise', 'event', 'life', 'interval', 'i', 'flood', 'joy', 'amazement', 'incredulity', 'mind', 'public', 'interest', 'glimpses', 'thoughts', 'actions', 'man', 'i', 'knowledge', 'i', 'duty', 'prohibition']\n",
      "['spring', 'year', 'london', 'world', 'murder', 'ronald', 'adair', 'circumstances', 'public', 'particulars', 'crime', 'police', 'investigation', 'deal', 'occasion', 'case', 'prosecution', 'facts', 'end', 'years', 'links', 'whole', 'chain', 'crime', 'interest', 'interest', 'nothing', 'sequel', 'shock', 'surprise', 'event', 'life', 'interval', 'flood', 'joy', 'amazement', 'incredulity', 'mind', 'public', 'interest', 'glimpses', 'thoughts', 'actions', 'man', 'knowledge', 'duty', 'prohibition', 'lips', 'third', 'month']\n"
     ]
    }
   ],
   "source": [
    "#Same for B2\n",
    "text_file_B2 = open(\"B2.txt\", \"r\")\n",
    "data = text_file_B2.read()\n",
    "text_file_B2.close()\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B2 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B2[:50])\n",
    "filtered_word_list_B2_NN = nouns_B2[:] #make a copy of the word_list\n",
    "for word in nouns_B2: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B2_NN.remove(word) # remove word from filtered_word_list if it is a stopword\n",
    "print(filtered_word_list_B2_NN[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tonis', 'name', 'marcel', 'life', 'baptism', 'days', 'shock', 'hair', 'snub', 'nose', 'tan', 'freckles', 'face', 'thick', 'pair', 'eyes', 'houris', 'paradise', 'mouth', 'teeth', 'smile', 'eyes', 'teeth', 'years', 'age', 'toni', 'man', 'worldof', 'world', 'gay', 'garrison', 'town', 'bienville', 'name', 'south', 'france', 'friends', 'foes', 'ladylove', 'plan', 'life', 'person', 'bienville', 'place', 'mother', 'madame', 'marcel', 'candy', 'shop', 'town']\n",
      "['tonis', 'name', 'marcel', 'life', 'baptism', 'days', 'shock', 'hair', 'snub', 'nose', 'tan', 'freckles', 'face', 'thick', 'pair', 'eyes', 'houris', 'paradise', 'mouth', 'teeth', 'smile', 'eyes', 'teeth', 'years', 'age', 'toni', 'man', 'worldof', 'world', 'gay', 'garrison', 'town', 'bienville', 'name', 'south', 'france', 'friends', 'foes', 'ladylove', 'plan', 'life', 'person', 'bienville', 'place', 'mother', 'madame', 'marcel', 'candy', 'shop', 'town']\n"
     ]
    }
   ],
   "source": [
    "#Same for B3\n",
    "text_file_B3 = open(\"B3.txt\", \"r\")\n",
    "data = text_file_B3.read()\n",
    "text_file_B3.close()\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B3 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B3[:50])\n",
    "filtered_word_list_B3_NN = nouns_B3[:] #make a copy of the word_list\n",
    "for word in nouns_B3: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B3_NN.remove(word) # remove word from filtered_word_list if it is a stopword\n",
    "print(filtered_word_list_B3_NN[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saw', 'was', 'crowded', 'paused', 'look', 'was', 'was', 'mingled', 'rising', 'lying', 'was', 'dominated', 'were', 'be', 'were', 'kneeling', 'had', 'paid', 'have', 'painted', 'were', 'paintbut', 'ended', 'was', 'differed']\n",
      "['saw', 'crowded', 'paused', 'look', 'mingled', 'rising', 'lying', 'dominated', 'kneeling', 'paid', 'painted', 'paintbut', 'ended', 'differed', 'ran', 'bore', 'tributes', 'beauty', 'supposebut', 'showered', 'slim', 'raddled', 'vulgar', 'gazing', 'darkened']\n",
      "['was', 'was', 'dismayed', 'has', 'learned', 'came', 'was', 'suppressed', 'was', 'was', 'bring', 'am', 'allowed', 'supply', 'missing', 'make', 'was', 'was', 'compared', 'afforded', 'find', 'thrilling', 'think', 'feeling', 'submerged']\n",
      "['dismayed', 'learned', 'came', 'suppressed', 'bring', 'allowed', 'supply', 'missing', 'make', 'compared', 'afforded', 'find', 'thrilling', 'think', 'feeling', 'submerged', 'let', 'say', 'shown', 'given', 'blame', 'shared', 'considered', 'barred', 'imagined']\n",
      "['was', 'was', 'called', 'was', 'was', 'had', 'were', 'had', 'appealing', 'have', 'belonged', 'was', 'smiled', 'was', 'began', 'ended', 'was', 'is', 'consisted', 'had', 'had', 'arranged', 'knew', 'be', 'kept']\n",
      "['called', 'appealing', 'belonged', 'smiled', 'began', 'ended', 'consisted', 'arranged', 'knew', 'kept', 'enjoyed', 'hadpaul', 'sonto', 'make', 'go', 'lie', 'watch', 'dancing', 'stumbling', 'toni', 'owing', 'built', 'take', 'set', 'learn']\n"
     ]
    }
   ],
   "source": [
    "#Extracting Verbs Now B1, B2, B3 respectively \n",
    "\n",
    "#B1\n",
    "\n",
    "text_file_B1 = open(\"B1.txt\", \"r\")\n",
    "data = text_file_B1.read()\n",
    "text_file_B1.close()\n",
    "# blob = TextBlob(data)\n",
    "# print(blob.noun_phrases[:100])\n",
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: pos[:2] == 'VB'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B1 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B1[:25])\n",
    "filtered_word_list_B1_VB = nouns_B1[:] #make a copy of the word_list\n",
    "for word in nouns_B1: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B1_VB.remove(word) # remove word from filtered_word_list if it is a stopword\n",
    "print(filtered_word_list_B1_VB[:25])\n",
    "#B2\n",
    "\n",
    "text_file_B2 = open(\"B2.txt\", \"r\")\n",
    "data = text_file_B2.read()\n",
    "text_file_B2.close()\n",
    "is_noun = lambda pos: pos[:2] == 'VB'\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B2 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B2[:25])\n",
    "filtered_word_list_B2_VB = nouns_B2[:] #make a copy of the word_list\n",
    "for word in nouns_B2: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B2_VB.remove(word) # remove word from filtered_word_list if it is a stopword\n",
    "print(filtered_word_list_B2_VB[:25])\n",
    "\n",
    "#B3\n",
    "text_file_B3 = open(\"B3.txt\", \"r\")\n",
    "data = text_file_B3.read()\n",
    "text_file_B3.close()\n",
    "is_noun = lambda pos: pos[:2] == 'VB'\n",
    "tokenized = nltk.word_tokenize(data)\n",
    "nouns_B3 = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print (nouns_B3[:25])\n",
    "filtered_word_list_B3_VB = nouns_B3[:] #make a copy of the word_list\n",
    "for word in nouns_B3: # iterate over word_list\n",
    "  if word in stopwords.words('english'): \n",
    "    filtered_word_list_B3_VB.remove(word) # remove word from filtered_word_list if it is a stopword\n",
    "print(filtered_word_list_B3_VB[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91941\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('name.v.01'), Synset('call.v.02'), Synset('call.v.03'), Synset('shout.v.02'), Synset('call.v.05'), Synset('visit.v.03'), Synset('call.v.07'), Synset('call.v.08'), Synset('call.v.09'), Synset('call.v.10'), Synset('call.v.11'), Synset('address.v.06'), Synset('call.v.13'), Synset('call.v.14'), Synset('bid.v.04'), Synset('call.v.16'), Synset('call.v.17'), Synset('predict.v.01'), Synset('call.v.19'), Synset('call.v.20'), Synset('call.v.21'), Synset('call.v.22'), Synset('call.v.23'), Synset('call.v.24'), Synset('call.v.25'), Synset('call.v.26'), Synset('call.v.27'), Synset('call.v.28')], [Synset('appeal.v.01'), Synset('appeal.v.02'), Synset('attract.v.02'), Synset('appeal.v.04'), Synset('invoke.v.02'), Synset('appealing.a.01'), Synset('sympathetic.a.04')], [Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.03'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('belong_to.v.01')]]\n"
     ]
    }
   ],
   "source": [
    "#Finding immediate categories (parent) that these words fall under in the WordNet\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "Categorie_B1_NN = []\n",
    "Categorie_B2_NN = []\n",
    "Categorie_B3_NN = []\n",
    "Categorie_B1_VB = []\n",
    "Categorie_B2_VB = []\n",
    "Categorie_B3_VB = []\n",
    "#We can look up words which are a part of the WordNet lexicon using the synsets()\n",
    "for i in filtered_word_list_B1_NN:\n",
    "    Categorie_B1_NN.append(wn.synsets(i))\n",
    "Categorie_B1_NN[:3]\n",
    "for i in filtered_word_list_B1_NN:\n",
    "    Categorie_B1_NN.append(wn.synsets(i))\n",
    "Categorie_B2_NN[:3]\n",
    "for i in filtered_word_list_B1_NN:\n",
    "    Categorie_B2_NN.append(wn.synsets(i))\n",
    "Categorie_B3_NN[:3]\n",
    "for i in filtered_word_list_B3_NN:\n",
    "    Categorie_B1_VB.append(wn.synsets(i))\n",
    "Categorie_B1_VB[:3]\n",
    "for i in filtered_word_list_B2_VB:\n",
    "    Categorie_B2_VB.append(wn.synsets(i))\n",
    "Categorie_B2_VB[:3]\n",
    "for i in filtered_word_list_B3_VB:\n",
    "    Categorie_B3_VB.append(wn.synsets(i))\n",
    "print(Categorie_B3_VB[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_eyre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16324/2252163509.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Making Histograms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_eyre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Counts of each tags in Jane Eyre is shown below \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tagged_eyre' is not defined"
     ]
    }
   ],
   "source": [
    "#Making Histograms\n",
    "from collections import Counter\n",
    "counts = Counter(tag for word,tag in tagged_eyre)\n",
    "print(\"Counts of each tags in Jane Eyre is shown below \")\n",
    "print()\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed3e2ff046ac52617a1704c0e2b654c30871d2bd634d62eb90e0c3e386c4cfdc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
